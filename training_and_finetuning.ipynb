{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN SOURCE MODELS IN MISTRAL\n",
    "\n",
    "- `Mistral-7B` - A 7B transformer model, fast-deployed and easily customisable. Small, yet very powerful for a variety of use cases.\n",
    "    - Performant in English and code \n",
    "    - 32k context window\n",
    "\n",
    "- `Mistral-8x7B` - A 7B sparse Mixture-of-Experts (SMoE). Uses 12.9B active parameters out of 45B total.\n",
    "    - Fluent in English, French, Italian, German, Spanish, and strong in code.\n",
    "    - 32k context window\n",
    "\n",
    "- `Mixtral-8x22B` - Currently the most performant open model. A 22B sparse Mixture-of-Experts (SMoE). Uses only 39B active parameters out of 141B.\n",
    "    - Fluent in English, French, Italian, German, Spanish, and strong in code.\n",
    "    - 64k context window.\n",
    "    - Native function calling capacities.\n",
    "    - Function calling and json mode available on our API endpoint.\n",
    "\n",
    "- There are also `Optimized models` in Mistral like `Mistral-small`, `Mistral-large` and `Mistral-Embed`. You can refer them in the [Mistral's Website](https://mistral.ai/technology/#models) and also in [huggingface](https://huggingface.co/mistralai)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIINETUNING `Mistral-8x7B Model`\n",
    "- I chose `Mistral-8x7B Model` model rather than `Mistral-7B` because the FlanV2_19k_smaples dataset which was preprocessed contains English along with other languages. So, it will be easier for the model to understand and train. Whereas the `Mistral-7B` was only for English and code. If we choose that It wouldn't perform that good. (It can perform if our sample size was huge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to your huggingface account\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "# To login you have to create your huggingface-access-token with WRITE permission.\n",
    "# you can also login through your terminal using the cli command --->  huggingface-cli login  and verify account using --> huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, GPTQConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "from transformers import LongformerTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'targets', 'task_source', 'task_name', 'template_type'],\n",
       "        num_rows: 19391\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset from huggingface which we pushed after preprocessing.\n",
    "dataset = load_dataset(\"karthiksagarn/FlanV2-2024\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>targets</th>\n",
       "      <th>task_source</th>\n",
       "      <th>task_name</th>\n",
       "      <th>template_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Write an article based on this summary:\\n\\nPur...</td>\n",
       "      <td>They should include long screws and wall ancho...</td>\n",
       "      <td>Flan2021</td>\n",
       "      <td>gem/wiki_lingua_english_en:1.1.0</td>\n",
       "      <td>zs_opt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Problem: What would be an example of an negati...</td>\n",
       "      <td>I go here about once every two weeks. They con...</td>\n",
       "      <td>Flan2021</td>\n",
       "      <td>yelp_polarity_reviews:0.2.0</td>\n",
       "      <td>fs_noopt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Input: Qingdao is located in northeast China, ...</td>\n",
       "      <td>Queens Park Rangers manager Harry Redknapp is ...</td>\n",
       "      <td>Flan2021</td>\n",
       "      <td>cnn_dailymail:3.4.0</td>\n",
       "      <td>fs_opt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Input: Steven Lippard, 7, was playing in the d...</td>\n",
       "      <td>A 21-year-old man in Chicago is charged with b...</td>\n",
       "      <td>Flan2021</td>\n",
       "      <td>cnn_dailymail:3.4.0</td>\n",
       "      <td>fs_opt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Here is a news article: In his last press conf...</td>\n",
       "      <td>– President Obama held the final press confere...</td>\n",
       "      <td>Flan2021</td>\n",
       "      <td>multi_news:1.0.0</td>\n",
       "      <td>zs_noopt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19386</th>\n",
       "      <td>Consider this response: Wa also occurs as a co...</td>\n",
       "      <td>DIALOG:\\nWhat is a The Burning City?\\n- The to...</td>\n",
       "      <td>Dialog</td>\n",
       "      <td>wiki_dialog_ii</td>\n",
       "      <td>fs_opt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19387</th>\n",
       "      <td>What came before. The bridge has been toll-fre...</td>\n",
       "      <td>-When was the New Hope Lambertville Bridge bui...</td>\n",
       "      <td>Dialog</td>\n",
       "      <td>wiki_dialog_ii</td>\n",
       "      <td>zs_opt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19388</th>\n",
       "      <td>Consider this response: To provide a high rate...</td>\n",
       "      <td>DIALOG:\\nWhat was George Lawrence Stone's tech...</td>\n",
       "      <td>Dialog</td>\n",
       "      <td>wiki_dialog_ii</td>\n",
       "      <td>fs_opt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19389</th>\n",
       "      <td>Read this response and predict the preceding d...</td>\n",
       "      <td>2-way dialog:\\n+ What is the difference betwee...</td>\n",
       "      <td>Dialog</td>\n",
       "      <td>wiki_dialog_ii</td>\n",
       "      <td>zs_opt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19390</th>\n",
       "      <td>Consider this response: One suggestion is that...</td>\n",
       "      <td>DIALOG:\\nWhat was the Atlanta International Po...</td>\n",
       "      <td>Dialog</td>\n",
       "      <td>wiki_dialog_ii</td>\n",
       "      <td>fs_opt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19391 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  inputs  \\\n",
       "0      Write an article based on this summary:\\n\\nPur...   \n",
       "1      Problem: What would be an example of an negati...   \n",
       "2      Input: Qingdao is located in northeast China, ...   \n",
       "3      Input: Steven Lippard, 7, was playing in the d...   \n",
       "4      Here is a news article: In his last press conf...   \n",
       "...                                                  ...   \n",
       "19386  Consider this response: Wa also occurs as a co...   \n",
       "19387  What came before. The bridge has been toll-fre...   \n",
       "19388  Consider this response: To provide a high rate...   \n",
       "19389  Read this response and predict the preceding d...   \n",
       "19390  Consider this response: One suggestion is that...   \n",
       "\n",
       "                                                 targets task_source  \\\n",
       "0      They should include long screws and wall ancho...    Flan2021   \n",
       "1      I go here about once every two weeks. They con...    Flan2021   \n",
       "2      Queens Park Rangers manager Harry Redknapp is ...    Flan2021   \n",
       "3      A 21-year-old man in Chicago is charged with b...    Flan2021   \n",
       "4      – President Obama held the final press confere...    Flan2021   \n",
       "...                                                  ...         ...   \n",
       "19386  DIALOG:\\nWhat is a The Burning City?\\n- The to...      Dialog   \n",
       "19387  -When was the New Hope Lambertville Bridge bui...      Dialog   \n",
       "19388  DIALOG:\\nWhat was George Lawrence Stone's tech...      Dialog   \n",
       "19389  2-way dialog:\\n+ What is the difference betwee...      Dialog   \n",
       "19390  DIALOG:\\nWhat was the Atlanta International Po...      Dialog   \n",
       "\n",
       "                              task_name template_type  \n",
       "0      gem/wiki_lingua_english_en:1.1.0        zs_opt  \n",
       "1           yelp_polarity_reviews:0.2.0      fs_noopt  \n",
       "2                   cnn_dailymail:3.4.0        fs_opt  \n",
       "3                   cnn_dailymail:3.4.0        fs_opt  \n",
       "4                      multi_news:1.0.0      zs_noopt  \n",
       "...                                 ...           ...  \n",
       "19386                    wiki_dialog_ii        fs_opt  \n",
       "19387                    wiki_dialog_ii        zs_opt  \n",
       "19388                    wiki_dialog_ii        fs_opt  \n",
       "19389                    wiki_dialog_ii        zs_opt  \n",
       "19390                    wiki_dialog_ii        fs_opt  \n",
       "\n",
       "[19391 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can also load the local data csv file using pandas\n",
    "dataset = pd.read_csv(\"Datasets/FlanV2_19k_samples.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reformat the dataset template to fit into the model.\n",
    "dataset[\"text\"] = dataset.apply(lambda row: \"###HUMAN: \" + row[\"inputs\"] + \" \" + \"###ASSISTANT: \" + row[\"targets\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use any tokenizer (for ex: bert-base-uncased, longformer-base), since we are using mistral model to finetune we use mistral's tokenizer only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "tokenizer = MistralTokenizer.v1()\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLORA Quantization using GPTQConfig\n",
    "quantization_config = GPTQConfig(bits=4, tokenizer=tokenizer, disable_exllama=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                                            \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map=\"auto\"\n",
    "                                            )\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Parameter Efficient Finetuning (PEFT) using Lora Config\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r = 16,\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    target_modules = [\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"Mistral_8x7B_FlanV2_Finetuned\",\n",
    "    hub_model_id=\"karthiksagarn/Mistral_8x7B_FlanV2_Finetuned\",\n",
    "    per_device_train_batch_size = 8,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    optim = \"paged_adamw_32bit\",\n",
    "    learning_rate = 2e-4,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_steps = 100,\n",
    "    num_train_epochs = 1,\n",
    "    # max_steps = 250,\n",
    "    fp16 = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    peft_config = peft_config,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 512,\n",
    "    tokenizer = tokenizer,\n",
    "    args = training_args,\n",
    "    packing=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PUSHING THE TRAINED MODLE TO HUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
